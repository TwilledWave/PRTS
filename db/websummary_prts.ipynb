{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ea5767a8-2b30-417c-bd8e-c83fd1b31270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this script summarizes the webpage from URL; writes its page content, meta data into a vector db\n",
    "\n",
    "from langchain.tools import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "import configparser, os\n",
    "config = configparser.ConfigParser()\n",
    "config.read('./keys.ini')\n",
    "os.environ['GOOGLE_API_KEY'] = config['GOOGLE']['GOOGLE_API_KEY']\n",
    "os.environ['GOOGLE_CSE_ID'] = config['GOOGLE']['GOOGLE_CSE_ID']\n",
    "openai_api_key = config['OPENAI']['OPENAI_API_KEY']\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "#web loader and split\n",
    "def web_loader_docs(link:str):\n",
    "    #input: link of the web page url\n",
    "    #web loader\n",
    "    loader = WebBaseLoader(link)\n",
    "    docs = loader.load()\n",
    "    #splitter\n",
    "    #text_splitter = RecursiveCharacterTextSplitter(chunk_size = 25000, chunk_overlap = 500)\n",
    "    #docs = text_splitter.split_documents(docs)\n",
    "    return docs\n",
    "\n",
    "\n",
    "#story summary chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "#summary the story dialogue from main page content of the URL\n",
    "def story_summary(docs):\n",
    "    #input: docs of the web page\n",
    "    # Define LLM chain\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\")\n",
    "    # Map\n",
    "    map_template = \"\"\"Summarize the dialogue in the docs.\n",
    "        {docs}\n",
    "        只输出中文。只输出总结，不需要评论故事。\n",
    "        输出:\"\"\"\n",
    "    map_prompt = PromptTemplate.from_template(map_template)\n",
    "    map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "    \n",
    "    # Reduce\n",
    "    reduce_template = \"\"\"依次陈列这几段故事情节。\n",
    "        {doc_summaries}\n",
    "        只输出中文。只输出故事，不需要评论故事。不要输出重复片段!\n",
    "        输出:\"\"\"\n",
    "    reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "    reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "    # Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "    combine_documents_chain = StuffDocumentsChain(\n",
    "        llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    "    )\n",
    "    # Combines and iteravely reduces the mapped documents\n",
    "    reduce_documents_chain = ReduceDocumentsChain(\n",
    "        # This is final chain that is called.\n",
    "        combine_documents_chain=combine_documents_chain,\n",
    "        # If documents exceed context for `StuffDocumentsChain`\n",
    "        collapse_documents_chain=combine_documents_chain,\n",
    "        # The maximum number of tokens to group documents into.\n",
    "        token_max=6000,\n",
    "    )\n",
    "\n",
    "    # Combining documents by mapping a chain over them, then combining results\n",
    "    map_reduce_chain = MapReduceDocumentsChain(\n",
    "        # Map chain\n",
    "        llm_chain=map_chain,\n",
    "        # Reduce chain\n",
    "        reduce_documents_chain=reduce_documents_chain,\n",
    "        # The variable name in the llm_chain to put the documents in\n",
    "        document_variable_name=\"docs\",\n",
    "        # Return the results of the map steps in the output\n",
    "        return_intermediate_steps=False,\n",
    "    )\n",
    "    #return the summary from the map and reduce procedure\n",
    "    return map_reduce_chain.run(docs)\n",
    "\n",
    "#use one 1 chain to summary the story dialogue from main page content of the URL\n",
    "def story_summary_stuff(docs):\n",
    "    #input: docs of the web page\n",
    "\n",
    "    # Define prompt\n",
    "    prompt_template = \"\"\"Summarize the dialogue in the text。\n",
    "    \"{text}\"\n",
    "    只输出中文。只输出故事，不需要评论故事。不要输出重复片段!\n",
    "    输出:\"\"\"\n",
    "    prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "    # Define LLM chain\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\")\n",
    "    llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    # Define StuffDocumentsChain\n",
    "    stuff_chain = StuffDocumentsChain(\n",
    "        llm_chain=llm_chain, document_variable_name=\"text\"\n",
    "    )\n",
    "    \n",
    "    return stuff_chain.run(docs)\n",
    "\n",
    "#summarize a web page from scappy meta data\n",
    "def websummary_meta(meta:dict, overwrite = False, runlm = True):\n",
    "    #input: meta data dictionary of the target page\n",
    "    link = meta['source']\n",
    "    #meta['characters'] = ','.join(meta['characters'])\n",
    "    if (meta['stage'] == None):\n",
    "        meta['stage'] = \"\"\n",
    "    #load vector db for summary data\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    from langchain.vectorstores import Chroma\n",
    "    db = Chroma(persist_directory=\"./cndb\", embedding_function=OpenAIEmbeddings())\n",
    "    #get the db existing id set\n",
    "    tmp = db.get()['ids']\n",
    "    this_db_list = [x.split(\"_\")[0] for x in tmp]\n",
    "    this_db_set = set(this_db_list)\n",
    "    #check whether the hash link is in the db already\n",
    "    import hashlib\n",
    "    this_id = str(int(hashlib.sha1(link.encode(\"utf-8\")).hexdigest(), 16) % (10 ** 8))\n",
    "    if this_id in this_db_set:\n",
    "        if overwrite == False:\n",
    "            return(link+\" link already in the db, skip\");\n",
    "        else:\n",
    "            index = this_db_list.index(this_id)\n",
    "            story = db.get()['documents'][index]\n",
    "    \n",
    "    #load\n",
    "    docs_org = web_loader_docs(link)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 20000, chunk_overlap = 500)\n",
    "    docs = text_splitter.split_documents(docs_org)\n",
    "\n",
    "    #find CG page from the webpage\n",
    "    cg_link = get_cg(docs_org)\n",
    "    meta['cg'] = cg_link\n",
    "\n",
    "    #summarize the story\n",
    "    if runlm == True:\n",
    "        if len(docs) > 2:\n",
    "            story = story_summary(docs);\n",
    "        else:\n",
    "            story = story_summary_stuff(docs);\n",
    "    \n",
    "    #write to vector db\n",
    "    meta['indexed'] = True\n",
    "    from langchain.docstore.document import Document\n",
    "    output_doc = Document(page_content=story, metadata=meta);\n",
    "    db.add_documents([output_doc], ids = [this_id])\n",
    "    \n",
    "    # #load the database for original page text\n",
    "    # db2 = Chroma(persist_directory=\"./arkpage\", embedding_function=OpenAIEmbeddings())\n",
    "    # text_splitter = RecursiveCharacterTextSplitter(chunk_size = 5000, chunk_overlap = 0)\n",
    "    # docs2 = text_splitter.split_documents(docs_org)\n",
    "    # this_list =[this_id + \"_\" + \"{0:0=4d}\".format(x) for x in range(len(docs2))]\n",
    "    # output_docs2 =[Document(page_content=docs2[x].page_content, metadata=meta) for x in range(len(docs2))]\n",
    "    # db2.add_documents(output_docs2, ids = this_list)\n",
    "    \n",
    "    return(output_doc)\n",
    "\n",
    "#return CG link from the story\n",
    "def get_cg(docs):\n",
    "    #get the page link from dialogue\n",
    "    regex=r'(?<=\\[Image\\(image=\\\")[\\w_]+'\n",
    "    pics = re.findall(regex, docs[0].page_content)\n",
    "    if len(pics)==0:\n",
    "        return(\"\");\n",
    "    link = \"https://prts.wiki/w/%E6%96%87%E4%BB%B6:Avg_\"+pics[0]+\".png\"\n",
    "    #get the 640px pic from the pic link\n",
    "    from urllib.request import urlopen\n",
    "    html_page = urlopen(link).read()\n",
    "    pics = re.findall(r'https://[\\w./-]+',str(html_page))\n",
    "    if len(pics)==0:\n",
    "        return(\"\");\n",
    "    if len(pics)>3:\n",
    "        if '640' in pics[2]:\n",
    "            return(pics[2]);\n",
    "    return(pics[0]);\n",
    "\n",
    "#main function, summarize a list of web pages from the scapy json\n",
    "def run_scapy(file = \"quotes.json\", limit = \"\", overwrite = False, runlm = True):\n",
    "    import json, time\n",
    "    # Opening JSON file\n",
    "    with open(file, encoding=\"utf-8\") as f:\n",
    "        scapy_list = json.load(f)\n",
    "\n",
    "    for l in range(len(scapy_list)):\n",
    "        if limit in scapy_list[l][\"stage\"]:\n",
    "            print(websummary_meta(scapy_list[l], overwrite, runlm))\n",
    "            scapy_list[l]['indexed'] = True;\n",
    "            with open(file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "                outfile.write(json.dumps(scapy_list))\n",
    "            time.sleep(16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e51c7dd8-3de1-4805-96d4-512be4980506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='1. 迈尔斯和杰西卡等人在达维镇等车，迈尔斯回忆过去的事情。\\n2. 车队首领宣布车队到达，海伦娜、伍德洛和杰西卡讨论是否要抢夺银行的钱。\\n3. 西尔维娅加入讨论并提供银行的地图。\\n4. 伍德洛和海伦娜对抢劫银行表示怀疑，但杰西卡决定加入他们。\\n5. 杰西卡拿出自己的铳，表明自己的决心。\\n6. 杰西卡找伍德洛哭诉，伍德洛安慰她。\\n7. 杰西卡突然发现异常情况，决定支持他们并拿出自己的武器。' metadata={'source': 'https://prts.wiki/index.php?title=CV-7_%E7%99%BD%E7%83%AD/BEG&action=edit', 'indexed': True, 'stage': 'CV-7 白热 行动前', 'cg': ''}\n",
      "page_content='芙兰卡、雷蛇、罗拉和杰西卡讨论了杰西卡的临时脱队申请和退队申请。杰西卡解释了她有一些私事需要处理，但她并不确定自己的选择是否值得。雷蛇表示不赞同杰西卡的选择，但最终同意了她的决定。随后，杰西卡和伍德洛、海伦娜、里昂一起进行了一次银行抢劫行动。在行动中，杰西卡展示了她的技能和勇气。最后，杰西卡收到了一张纸条，上面写着一些鼓励的话和祝福。\\n\\n西尔维娅告诉大家金库内还有两道门，一道栅栏门和一道重达数十吨的保险门。栅栏门需要密码打开，密码分为三段，行长和经理各掌握一段，还有一段由密码器随机生成的数字。海伦娜和里昂成功打开了保险箱，但里昂担心伍德洛那边的情况。接着，伍德洛和杰西卡遇到了银行经理，杰西卡要求经理告知密码，否则他们会被杀死。经理嘲笑他们的贪婪，并预言他们会被银行追踪并分食。最后，经理告诉他们密码，伍德洛准备引爆炸药。爆炸后，他们发现大量的钞票，但伍德洛提醒大家没有时间感慨，要开始装钱。杰西卡感叹自己第一次对金钱如此渴望。' metadata={'source': 'https://prts.wiki/index.php?title=CV-7_%E7%99%BD%E7%83%AD/END&action=edit', 'indexed': True, 'stage': 'CV-7 白热 行动后', 'cg': 'https://prts.wiki/images/thumb/e/e9/Avg_42_i02.png/640px-Avg_42_i02.png'}\n"
     ]
    }
   ],
   "source": [
    "run_scapy(file = \"prts101023.json\", limit = \"CV-7\", overwrite = True, runlm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d6b83611-ac5a-4236-b5d7-4d3d477317bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "db = Chroma(persist_directory=\"./cndb\", embedding_function=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "798611c0-b9f5-4e90-84f6-9ab2da9331ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cg': 'https://prts.wiki/images/thumb/f/f6/Avg_32_i03.png/640px-Avg_32_i03.png',\n",
       " 'indexed': True,\n",
       " 'source': 'https://prts.wiki/index.php?title=13-6_%E5%85%B8%E8%8C%83%E4%B9%8B%E5%90%8D/END&action=edit',\n",
       " 'stage': '13-6 典范之名 行动后'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.get()['metadatas'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada65f5-de2b-4013-8303-21bfa8d160a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
